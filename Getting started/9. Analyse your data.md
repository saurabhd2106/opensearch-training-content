Analysis 

How Open search process the data when you index it? When you index any document in the open search the documents are send to the analysis process

An analysis is a process of converting text, like the body of a document into tokens or terms and later these terms are added into the inverted index for searching

With the help of this inverted indexes, open search acheives the very first full text searches. 

Whenever you index a document in the open search it goes through a number of steps for every analysed feild before the document is added into the index 


# Building custom Analyzer

PUT /blogpost
{
  "settings": {
    "analysis": {
      "analyzer": {
        "custom_analyser":{
          "type" : "custom",
          "char_filter" : ["html_strip"],
          "tokenizer" : "standard",
          "filter" : ["lowercase"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title" : {"type": "text"},
      "content" : {"type": "text", "analyzer": "custom_analyser"},
      "published_date" : {"type": "date"},
      "no_of_like" : {"type": "text"},
      "tags" : {"type": "text"},
      "status" : {"type": "text"}
    }
  }
}

GET /blogpost

POST /blogpost/_analyze
{
  "analyzer": "custom_analyser",
  "text": "This is a HTML <p> word </p> & has characters"
}

# Building a custom Analyzer 2

PUT /blogpost
{
  "settings": {
    "analysis": {
      "analyzer": {
        "custom_analyser":{
          "type" : "custom",
          "tokenizer": "punctuation_tokenizer",
          "char_filter": ["symbol"],
          "filter": ["my_stop", "lowercase"]
          
        }
      },
      "tokenizer": {
        "punctuation_tokenizer" : {
          "type": "pattern",
          "pattern": "[.,!? ]"
        }
      },
      "char_filter": {
        "symbol":{
          "type" : "mapping",
          "mappings": [ "& => and", ":) => happy", ":( => sad" ]
           
        }
      },
      "filter": {
        "my_stop": {
          "type": "stop",
          "stopwords" : "_english_"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title" : {"type": "text"},
      "content" : {"type": "text", "analyzer": "custom_analyser"},
      "published_date" : {"type": "date"},
      "no_of_like" : {"type": "text"},
      "tags" : {"type": "text"},
      "status" : {"type": "text"}
    }
  }
}

GET /blogpost

POST /blogpost/_analyze
{
  "analyzer": "custom_analyser",
  "text": "This is a HTML :) word :( & has characters"
} 

# NGRAM Analyzer


DELETE /blogpost

POST /_analyze
{
  "tokenizer": "ngram",
  "text": "Search"
}

POST /ngram_index/_analyze
{
  "analyzer": "ngram_analyzer",
  "text": "Search"
}

DELETE ngram_index

PUT ngram_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "ngram_analyzer" : {
          "tokenizer" :"ngram_tokenizer"
        }
      },
      "tokenizer": {
        "ngram_tokenizer" :{
          "type" : "ngram",
          "min_gram" : 2,
          "max_gram" : 3
        }
      }
    }
  }
}

# Edge Analyser

POST /edge_index/_analyze
{
  "analyzer": "edge_analyzer",
  "text": "Search"
}


PUT edge_index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "edge_analyzer" : {
          "tokenizer" :"edge_tokenizer"
        }
      },
      "tokenizer": {
        "edge_tokenizer" :{
          "type" : "edge_ngram",
          "min_gram" : 2,
          "max_gram" : 6
        }
      }
    }
  }
}